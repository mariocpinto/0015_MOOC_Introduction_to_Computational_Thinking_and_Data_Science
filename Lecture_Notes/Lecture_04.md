### Lecture 4: Stochastic Programming and Statistical Thinking

* [Introduction to Stochastic Programming](https://www.youtube.com/watch?v=ADB7EXNTVqs)

* [Law of Large Numbers](https://www.youtube.com/watch?v=Gauzb9JNaMQ)
  * Also known as Bernoulli's Law:
  
  > In repeated independent tests with the same actual probability p of a particular outcome in each test,
  the chance that the fraction of times that outcome occurs differs from p converges to zero as the number of trials goes to infinity.

  * Gambler's fallacy:
  
  >If deviations from expected behavior occur, these deviations are likely to be evened out by opposite deviations in the future. 
  
* [How much is enough?](https://www.youtube.com/watch?v=wbjZ332wGEo)

* [Standard Deviations and Histograms](https://www.youtube.com/watch?v=ky6rSZOoBws)
  * Coefficient of variation = standard deviation / mean.
  * Plotting histograms with `pyplot.hist` command.

<br>

[Back to course notes](../Course_Notes.md)
